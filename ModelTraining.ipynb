{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59971ca5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9dee5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training and validation generators\n",
    "\n",
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "train_datagenerator = ImageDataGenerator(rescale=1./255)\n",
    "test_datagenerator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a264a9ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Building the CNN architecture\n",
    "# with four conv2D layers, two dense layers and one flatten layer\n",
    "\n",
    "recognition_model = Sequential()\n",
    "\n",
    "recognition_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "recognition_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "recognition_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "recognition_model.add(Dropout(0.25))\n",
    "\n",
    "recognition_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "recognition_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "recognition_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "recognition_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "recognition_model.add(Dropout(0.25))\n",
    "\n",
    "recognition_model.add(Flatten())\n",
    "recognition_model.add(Dense(1024, activation='relu'))\n",
    "recognition_model.add(Dropout(0.5))\n",
    "recognition_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab66114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 2,345,607\n",
      "Trainable params: 2,345,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Getting Model Summary\n",
    "\n",
    "recognition_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3c30e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Disable OpenCL\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be16edc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "\n",
    "recognition_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001, decay=1e-6),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d870bdaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "448/448 [==============================] - 196s 434ms/step - loss: 1.8224 - accuracy: 0.2537 - val_loss: 1.7200 - val_accuracy: 0.3249\n",
      "Epoch 2/60\n",
      "448/448 [==============================] - 169s 376ms/step - loss: 1.6697 - accuracy: 0.3482 - val_loss: 1.5654 - val_accuracy: 0.4079\n",
      "Epoch 3/60\n",
      "448/448 [==============================] - 176s 392ms/step - loss: 1.5535 - accuracy: 0.3995 - val_loss: 1.4613 - val_accuracy: 0.4422\n",
      "Epoch 4/60\n",
      "448/448 [==============================] - 178s 398ms/step - loss: 1.4751 - accuracy: 0.4366 - val_loss: 1.4009 - val_accuracy: 0.4690\n",
      "Epoch 5/60\n",
      "448/448 [==============================] - 268s 598ms/step - loss: 1.4121 - accuracy: 0.4574 - val_loss: 1.3517 - val_accuracy: 0.4867\n",
      "Epoch 6/60\n",
      "448/448 [==============================] - 233s 519ms/step - loss: 1.3671 - accuracy: 0.4796 - val_loss: 1.3221 - val_accuracy: 0.4965\n",
      "Epoch 7/60\n",
      "448/448 [==============================] - 163s 362ms/step - loss: 1.3138 - accuracy: 0.5008 - val_loss: 1.2813 - val_accuracy: 0.5098\n",
      "Epoch 8/60\n",
      "448/448 [==============================] - 137s 306ms/step - loss: 1.2703 - accuracy: 0.5207 - val_loss: 1.2521 - val_accuracy: 0.5202\n",
      "Epoch 9/60\n",
      "448/448 [==============================] - 144s 320ms/step - loss: 1.2383 - accuracy: 0.5251 - val_loss: 1.2309 - val_accuracy: 0.5308\n",
      "Epoch 10/60\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 1.2071 - accuracy: 0.5461 - val_loss: 1.2025 - val_accuracy: 0.5413\n",
      "Epoch 11/60\n",
      "448/448 [==============================] - 154s 343ms/step - loss: 1.1789 - accuracy: 0.5607 - val_loss: 1.1921 - val_accuracy: 0.5449\n",
      "Epoch 12/60\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 1.1537 - accuracy: 0.5676 - val_loss: 1.1747 - val_accuracy: 0.5526\n",
      "Epoch 13/60\n",
      "448/448 [==============================] - 152s 340ms/step - loss: 1.1243 - accuracy: 0.5769 - val_loss: 1.1663 - val_accuracy: 0.5537\n",
      "Epoch 14/60\n",
      "448/448 [==============================] - 133s 298ms/step - loss: 1.0982 - accuracy: 0.5907 - val_loss: 1.1503 - val_accuracy: 0.5587\n",
      "Epoch 15/60\n",
      "448/448 [==============================] - 129s 289ms/step - loss: 1.0669 - accuracy: 0.6019 - val_loss: 1.1395 - val_accuracy: 0.5636\n",
      "Epoch 16/60\n",
      "448/448 [==============================] - 131s 291ms/step - loss: 1.0603 - accuracy: 0.6048 - val_loss: 1.1167 - val_accuracy: 0.5749\n",
      "Epoch 17/60\n",
      "448/448 [==============================] - 132s 294ms/step - loss: 1.0219 - accuracy: 0.6230 - val_loss: 1.1267 - val_accuracy: 0.5691\n",
      "Epoch 18/60\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 0.9963 - accuracy: 0.6326 - val_loss: 1.1112 - val_accuracy: 0.5805\n",
      "Epoch 19/60\n",
      "448/448 [==============================] - 129s 287ms/step - loss: 0.9692 - accuracy: 0.6405 - val_loss: 1.0992 - val_accuracy: 0.5841\n",
      "Epoch 20/60\n",
      "448/448 [==============================] - 130s 289ms/step - loss: 0.9462 - accuracy: 0.6484 - val_loss: 1.0839 - val_accuracy: 0.5872\n",
      "Epoch 21/60\n",
      "448/448 [==============================] - 129s 289ms/step - loss: 0.9146 - accuracy: 0.6664 - val_loss: 1.0820 - val_accuracy: 0.5942\n",
      "Epoch 22/60\n",
      "448/448 [==============================] - 132s 294ms/step - loss: 0.9086 - accuracy: 0.6677 - val_loss: 1.0794 - val_accuracy: 0.5946\n",
      "Epoch 23/60\n",
      "448/448 [==============================] - 132s 293ms/step - loss: 0.8742 - accuracy: 0.6773 - val_loss: 1.0796 - val_accuracy: 0.5965\n",
      "Epoch 24/60\n",
      "448/448 [==============================] - 130s 290ms/step - loss: 0.8576 - accuracy: 0.6826 - val_loss: 1.0824 - val_accuracy: 0.5970\n",
      "Epoch 25/60\n",
      "448/448 [==============================] - 130s 290ms/step - loss: 0.8264 - accuracy: 0.6976 - val_loss: 1.0665 - val_accuracy: 0.6059\n",
      "Epoch 26/60\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.8113 - accuracy: 0.7024 - val_loss: 1.0688 - val_accuracy: 0.6056\n",
      "Epoch 27/60\n",
      "448/448 [==============================] - 132s 296ms/step - loss: 0.7880 - accuracy: 0.7164 - val_loss: 1.0674 - val_accuracy: 0.6032\n",
      "Epoch 28/60\n",
      "448/448 [==============================] - 139s 310ms/step - loss: 0.7584 - accuracy: 0.7216 - val_loss: 1.0809 - val_accuracy: 0.6060\n",
      "Epoch 29/60\n",
      "448/448 [==============================] - 133s 297ms/step - loss: 0.7381 - accuracy: 0.7311 - val_loss: 1.0879 - val_accuracy: 0.6071\n",
      "Epoch 30/60\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.7172 - accuracy: 0.7400 - val_loss: 1.0659 - val_accuracy: 0.6064\n",
      "Epoch 31/60\n",
      "448/448 [==============================] - 130s 290ms/step - loss: 0.6892 - accuracy: 0.7481 - val_loss: 1.0796 - val_accuracy: 0.6105\n",
      "Epoch 32/60\n",
      "448/448 [==============================] - 133s 297ms/step - loss: 0.6719 - accuracy: 0.7576 - val_loss: 1.0684 - val_accuracy: 0.6104\n",
      "Epoch 33/60\n",
      "448/448 [==============================] - 129s 288ms/step - loss: 0.6520 - accuracy: 0.7621 - val_loss: 1.0826 - val_accuracy: 0.6137\n",
      "Epoch 34/60\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.6459 - accuracy: 0.7651 - val_loss: 1.0942 - val_accuracy: 0.6137\n",
      "Epoch 35/60\n",
      "448/448 [==============================] - 131s 293ms/step - loss: 0.6236 - accuracy: 0.7729 - val_loss: 1.0867 - val_accuracy: 0.6154\n",
      "Epoch 36/60\n",
      "448/448 [==============================] - 129s 287ms/step - loss: 0.5926 - accuracy: 0.7863 - val_loss: 1.0928 - val_accuracy: 0.6145\n",
      "Epoch 37/60\n",
      "448/448 [==============================] - 130s 290ms/step - loss: 0.5772 - accuracy: 0.7893 - val_loss: 1.1008 - val_accuracy: 0.6177\n",
      "Epoch 38/60\n",
      "448/448 [==============================] - 157s 350ms/step - loss: 0.5528 - accuracy: 0.7999 - val_loss: 1.0959 - val_accuracy: 0.6136\n",
      "Epoch 39/60\n",
      "448/448 [==============================] - 156s 347ms/step - loss: 0.5421 - accuracy: 0.8048 - val_loss: 1.1154 - val_accuracy: 0.6138\n",
      "Epoch 40/60\n",
      "448/448 [==============================] - 128s 286ms/step - loss: 0.5166 - accuracy: 0.8109 - val_loss: 1.1252 - val_accuracy: 0.6155\n",
      "Epoch 41/60\n",
      "448/448 [==============================] - 122s 273ms/step - loss: 0.5127 - accuracy: 0.8149 - val_loss: 1.1289 - val_accuracy: 0.6179\n",
      "Epoch 42/60\n",
      "448/448 [==============================] - 136s 303ms/step - loss: 0.4875 - accuracy: 0.8218 - val_loss: 1.1662 - val_accuracy: 0.6191\n",
      "Epoch 43/60\n",
      "448/448 [==============================] - 134s 298ms/step - loss: 0.4689 - accuracy: 0.8292 - val_loss: 1.1409 - val_accuracy: 0.6214\n",
      "Epoch 44/60\n",
      "448/448 [==============================] - 146s 326ms/step - loss: 0.4439 - accuracy: 0.8415 - val_loss: 1.1613 - val_accuracy: 0.6189\n",
      "Epoch 45/60\n",
      "448/448 [==============================] - 141s 316ms/step - loss: 0.4383 - accuracy: 0.8428 - val_loss: 1.1646 - val_accuracy: 0.6194\n",
      "Epoch 46/60\n",
      "448/448 [==============================] - 131s 293ms/step - loss: 0.4195 - accuracy: 0.8484 - val_loss: 1.1620 - val_accuracy: 0.6217\n",
      "Epoch 47/60\n",
      "448/448 [==============================] - 132s 295ms/step - loss: 0.4100 - accuracy: 0.8519 - val_loss: 1.1914 - val_accuracy: 0.6197\n",
      "Epoch 48/60\n",
      "448/448 [==============================] - 130s 291ms/step - loss: 0.3951 - accuracy: 0.8566 - val_loss: 1.1721 - val_accuracy: 0.6201\n",
      "Epoch 49/60\n",
      "448/448 [==============================] - 140s 312ms/step - loss: 0.3859 - accuracy: 0.8593 - val_loss: 1.1995 - val_accuracy: 0.6198\n",
      "Epoch 50/60\n",
      "448/448 [==============================] - 132s 295ms/step - loss: 0.3667 - accuracy: 0.8678 - val_loss: 1.1945 - val_accuracy: 0.6144\n",
      "Epoch 51/60\n",
      "448/448 [==============================] - 130s 290ms/step - loss: 0.3574 - accuracy: 0.8701 - val_loss: 1.2387 - val_accuracy: 0.6162\n",
      "Epoch 52/60\n",
      "448/448 [==============================] - 130s 291ms/step - loss: 0.3467 - accuracy: 0.8732 - val_loss: 1.2253 - val_accuracy: 0.6166\n",
      "Epoch 53/60\n",
      "448/448 [==============================] - 132s 294ms/step - loss: 0.3408 - accuracy: 0.8789 - val_loss: 1.2520 - val_accuracy: 0.6177\n",
      "Epoch 54/60\n",
      "448/448 [==============================] - 132s 294ms/step - loss: 0.3275 - accuracy: 0.8836 - val_loss: 1.2566 - val_accuracy: 0.6169\n",
      "Epoch 55/60\n",
      "448/448 [==============================] - 127s 284ms/step - loss: 0.3273 - accuracy: 0.8803 - val_loss: 1.2505 - val_accuracy: 0.6158\n",
      "Epoch 56/60\n",
      "448/448 [==============================] - 125s 278ms/step - loss: 0.3114 - accuracy: 0.8897 - val_loss: 1.2610 - val_accuracy: 0.6228\n",
      "Epoch 57/60\n",
      "448/448 [==============================] - 126s 282ms/step - loss: 0.3071 - accuracy: 0.8875 - val_loss: 1.2757 - val_accuracy: 0.6194\n",
      "Epoch 58/60\n",
      "448/448 [==============================] - 130s 289ms/step - loss: 0.2956 - accuracy: 0.8962 - val_loss: 1.2785 - val_accuracy: 0.6180\n",
      "Epoch 59/60\n",
      "448/448 [==============================] - 123s 275ms/step - loss: 0.3027 - accuracy: 0.8917 - val_loss: 1.2921 - val_accuracy: 0.6187\n",
      "Epoch 60/60\n",
      "448/448 [==============================] - 124s 277ms/step - loss: 0.2832 - accuracy: 0.9021 - val_loss: 1.2732 - val_accuracy: 0.6175\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "recognition_model_info = recognition_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=60,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197e09d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recognition_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bc20d08e7ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Saving the trained Model Weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrecognition_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recognition_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'recognition_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving the trained Model Weights\n",
    "\n",
    "recognition_model.save_weights('recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73855370",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cccc1966ff1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecognition_model_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecognition_model_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Summarize history for accuracy\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(recognition_model_info.history['accuracy'])\n",
    "plt.plot(recognition_model_info.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.plot(recognition_model_info.history['loss'])\n",
    "plt.plot(recognition_model_info.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d2eff6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Facial emotion Dictonary with values\n",
    "\n",
    "facial_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe7ace4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-03a99d417408>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Start the webcam feed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Find haar cascade to draw bounding box around face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Start the webcam feed\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        facial_prediction = recognition_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(facial_prediction))\n",
    "        cv2.putText(frame, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
